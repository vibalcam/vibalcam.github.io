---
layout: post
title:  "Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data"
date:   2025-02-25
image: /images/dft_pdf.png
categories: preprint
authors: "Siqi Guo, Ilgee Hong, <strong>Vicente Balmaseda</strong>, Tuo Zhao, Tianbao Yang"
arxiv: https://arxiv.org/abs/2502.18679
code: https://github.com/PenGuln/DFT
---
We introduce *Discriminative Fine-Tuning (DFT)*, an approach for fine-tuning LLMs without preference data or reward models. Unlike Supervised Fine-Tuning (SFT), DFT adopts a discriminative paradigm to efficiently optimize the likelihood of an answer among all possible outputs given an input.
