---
layout: post
title:  "Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data"
date:   2025-02-25
image: /images/dft_pdf.png
categories: research
authors: "Siqi Guo, Ilgee Hong, <strong>Vicente Balmaseda</strong>,  Changlong Yu, Liang Qiu, Xin Liu, Haoming Jiang, Tuo Zhao, Tianbao Yang"
venue: "ICML 2025 &mdash; Forty-second International Conference on Machine Learning"
arxiv: https://arxiv.org/abs/2502.18679
# paper:
venueSite: https://icml.cc/virtual/2025/poster/46619
poster: assets/icml2025-dft-poster.png
code: https://github.com/PenGuln/DFT
topic: llm
---
We introduce *Discriminative Fine-Tuning (DFT)*, an approach for fine-tuning LLMs without preference data or reward models. Unlike Supervised Fine-Tuning (SFT), DFT adopts a discriminative paradigm to efficiently optimize the likelihood of an answer among all possible outputs given an input.
