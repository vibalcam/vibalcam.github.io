---
layout: post
title:  "Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning"
date:   2025-07-09
image: /images/glofnd2.png
categories: research
authors: "<strong>Vicente Balmaseda</strong>, Bokun Wang, Ching-Long Lin, Tianbao Yang"
venue: "Accepted to Forty-second International Conference on Machine Learning (ICML 2025)"
arxiv: https://arxiv.org/abs/2502.20612
# paper:
venueSite: https://icml.cc/virtual/2025/poster/45576
poster: assets/icml2025-glofnd-poster.pdf
code: https://github.com/vibalcam/GloFND
---
<!-- Vancouver, Canada -->
In self-supervised contrastive learning, negative samples may inadvertently share semantics with the anchor, leading to false negatives that degrade representation quality. We introduce *GloFND*, a scalable optimization-based method that discovers false negatives during training. It identifies them globally across the dataset while keeping computation confined to each mini-batch and independent of dataset size. *GloFND* integrates seamlessly with unimodal and bimodal contrastive frameworks, consistently improving representation quality with minimal overhead.
<!-- In self-supervised contrastive learning, negative samples may inadvertently share semantics with the anchor, leading to false negatives that degrade representation quality. We introduce GLOFND, a scalable optimization-based method that discovers false negatives globally across the dataset by learning dynamic, per-sample thresholds on the fly. Unlike batch-local approaches, GLOFND operates independently of batch size and integrates seamlessly into existing contrastive frameworks. Extensive experiments in unimodal, bimodal, and semi-supervised settings show that GLOFND consistently improves false negative detection and downstream performanceâ€”with negligible overhead. -->
